"""
XML Batch Comparison Tool
è¤‡æ•°ã®ã‚²ãƒ¼ãƒ ãƒãƒ‹ãƒ¥ã‚¢ãƒ«XMLã‚’ä¸€æ‹¬æ¯”è¼ƒ
"""

import sys
import os
from pathlib import Path
from xml_section_comparison import XMLSectionComparison
from typing import List, Dict


class XMLBatchComparison:
    """è¤‡æ•°XMLãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€æ‹¬æ¯”è¼ƒã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, ai_dir: str, human_dir: str, game_ids: List[str] = None):
        self.ai_dir = Path(ai_dir)
        self.human_dir = Path(human_dir)
        self.game_ids = game_ids if game_ids else self._auto_detect_game_ids()
        self.results = []
    
    def _auto_detect_game_ids(self) -> List[str]:
        """human_dirã‹ã‚‰_human.xmlãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡ºã—ã¦ã‚²ãƒ¼ãƒ IDãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ"""
        print(f"ğŸ” Auto-detecting game IDs from: {self.human_dir}")
        
        game_ids = []
        for file_path in self.human_dir.glob("*_human.xml"):
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰_human.xmlã‚’é™¤ã„ã¦ã‚²ãƒ¼ãƒ IDã‚’å–å¾—
            game_id = file_path.stem.replace('_human', '')
            
            # å¯¾å¿œã™ã‚‹AIãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
            ai_file = self.ai_dir / f"{game_id}_labeled.xml"
            if ai_file.exists():
                game_ids.append(game_id)
                print(f"  âœ… Found: {game_id}")
            else:
                print(f"  âš ï¸  Skipped: {game_id} (no AI file)")
        
        print(f"\nğŸ“Š Total games found: {len(game_ids)}")
        return sorted(game_ids)
        
    def run_batch_comparison(self):
        """ãƒãƒƒãƒæ¯”è¼ƒã‚’å®Ÿè¡Œ"""
        print("=" * 80)
        print("XML BATCH COMPARISON - Multiple Game Manuals")
        print("=" * 80)
        print(f"\nComparing {len(self.game_ids)} game manual(s)...\n")
        
        for game_id in self.game_ids:
            print(f"\n{'='*80}")
            print(f"Processing: {game_id}")
            print(f"{'='*80}")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æ§‹ç¯‰
            ai_file = self.ai_dir / f"{game_id}_labeled.xml"
            human_file = self.human_dir / f"{game_id}_human.xml"
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
            if not ai_file.exists():
                print(f"âš ï¸  AI file not found: {ai_file}")
                continue
            if not human_file.exists():
                print(f"âš ï¸  Human file not found: {human_file}")
                continue
            
            # æ¯”è¼ƒã‚’å®Ÿè¡Œ
            try:
                comparator = XMLSectionComparison(str(ai_file), str(human_file))
                
                # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ã¨æŠ½å‡º
                comparator.load_xml_files()
                ai_sections = comparator.extract_section_tree(comparator.ai_tree, "AI")
                human_sections = comparator.extract_section_tree(comparator.human_tree, "Human")
                
                # çµ±è¨ˆè¨ˆç®—
                ai_stats = comparator.calculate_tree_statistics(ai_sections, "AI")
                human_stats = comparator.calculate_tree_statistics(human_sections, "Human")
                
                # ãƒšãƒ¼ã‚¸å˜ä½ã®æ¯”è¼ƒ
                page_comparisons = comparator.align_sections_by_page(ai_sections, human_sections)
                
                # å…¨ä½“ãƒ¡ãƒˆãƒªã‚¯ã‚¹
                overall_metrics = comparator.calculate_overall_metrics(page_comparisons)
                
                # çµæœã‚’ä¿å­˜
                self.results.append({
                    'game_id': game_id,
                    'ai_stats': ai_stats,
                    'human_stats': human_stats,
                    'page_comparisons': page_comparisons,
                    'overall_metrics': overall_metrics
                })
                
                # å€‹åˆ¥ãƒ¬ãƒãƒ¼ãƒˆã‚’è¡¨ç¤º
                report = comparator.generate_report(ai_stats, human_stats, 
                                                   page_comparisons, overall_metrics)
                print("\n" + report)
                
                # å€‹åˆ¥ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                output_file = f"comparison_report_{game_id}.txt"
                with open(output_file, 'w', encoding='utf-8') as f:
                    f.write(report)
                print(f"\nğŸ’¾ Individual report saved to: {output_file}")
                
            except Exception as e:
                print(f"âŒ Error processing {game_id}: {str(e)}")
                import traceback
                traceback.print_exc()
                continue
        
        # çµ±åˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
        if self.results:
            self.generate_summary_report()
    
    def generate_summary_report(self):
        """å…¨ã‚²ãƒ¼ãƒ ã®çµ±åˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        print("\n" + "=" * 80)
        print("SUMMARY REPORT - All Games")
        print("=" * 80)
        
        report = []
        report.append("=" * 80)
        report.append("BATCH COMPARISON SUMMARY REPORT")
        report.append(f"Total games analyzed: {len(self.results)}")
        report.append("=" * 80)
        
        # å„ã‚²ãƒ¼ãƒ ã®ã‚µãƒãƒªãƒ¼
        report.append("\nğŸ“Š PER-GAME SUMMARY:")
        report.append(f"{'Game ID':<20} {'AI Secs':<10} {'Hum Secs':<10} {'F1-Score':<10} {'Edit Dist':<12}")
        report.append("-" * 80)
        
        total_f1 = 0
        total_edit_dist = 0
        
        for result in self.results:
            game_id = result['game_id']
            ai_total = result['ai_stats']['total_sections']
            human_total = result['human_stats']['total_sections']
            metrics = result['overall_metrics']
            
            # F1ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
            tp = metrics['total_exact_matches']
            ai_types = metrics['total_ai_types']
            human_types = metrics['total_human_types']
            
            precision = tp / ai_types if ai_types > 0 else 0
            recall = tp / human_types if human_types > 0 else 0
            f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
            
            norm_edit_dist = metrics['normalized_edit_distance']
            
            total_f1 += f1_score
            total_edit_dist += norm_edit_dist
            
            report.append(
                f"{game_id:<20} "
                f"{ai_total:<10} "
                f"{human_total:<10} "
                f"{f1_score:<10.3f} "
                f"{norm_edit_dist:<12.3f}"
            )
        
        # å¹³å‡å€¤
        avg_f1 = total_f1 / len(self.results)
        avg_edit_dist = total_edit_dist / len(self.results)
        
        report.append("-" * 80)
        report.append(f"{'AVERAGE':<20} {'':<10} {'':<10} {avg_f1:<10.3f} {avg_edit_dist:<12.3f}")
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ—ã®é›†è¨ˆ
        report.append("\nğŸ“‹ AGGREGATED SECTION TYPE STATISTICS:")
        
        # å…¨ã‚²ãƒ¼ãƒ ã®ã‚¿ã‚¤ãƒ—ã‚’é›†è¨ˆ
        all_ai_types = {}
        all_human_types = {}
        
        for result in self.results:
            for stype, count in result['ai_stats']['type_distribution'].items():
                all_ai_types[stype] = all_ai_types.get(stype, 0) + count
            for stype, count in result['human_stats']['type_distribution'].items():
                all_human_types[stype] = all_human_types.get(stype, 0) + count
        
        all_types = sorted(set(all_ai_types.keys()) | set(all_human_types.keys()))
        
        report.append(f"{'Type':<25} {'Total AI':<12} {'Total Human':<12} {'Diff':<10}")
        report.append("-" * 60)
        
        for stype in all_types:
            ai_count = all_ai_types.get(stype, 0)
            human_count = all_human_types.get(stype, 0)
            diff = ai_count - human_count
            diff_str = f"{diff:+d}" if diff != 0 else "0"
            report.append(f"{stype:<25} {ai_count:<12} {human_count:<12} {diff_str:<10}")
        
        # ç·è©•
        report.append("\nğŸ’¡ OVERALL ASSESSMENT:")
        if avg_f1 >= 0.8:
            report.append("  âœ… å„ªç§€: AIç”Ÿæˆã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ§‹é€ ã¯å…¨ä½“çš„ã«éå¸¸ã«é«˜å“è³ª")
        elif avg_f1 >= 0.6:
            report.append("  âš ï¸  è‰¯å¥½: AIç”Ÿæˆã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ§‹é€ ã¯æ¦‚ã­é©åˆ‡")
        elif avg_f1 >= 0.4:
            report.append("  âš ï¸  è¦æ”¹å–„: ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ§‹é€ ã®ç²¾åº¦å‘ä¸ŠãŒæ¨å¥¨ã•ã‚Œã‚‹")
        else:
            report.append("  âŒ ä¸ååˆ†: ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ§‹é€ ã®å¤§å¹…ãªæ”¹å–„ãŒå¿…è¦")
        
        report.append(f"\nå¹³å‡F1ã‚¹ã‚³ã‚¢: {avg_f1:.3f}")
        report.append(f"å¹³å‡æ­£è¦åŒ–ç·¨é›†è·é›¢: {avg_edit_dist:.3f}")
        
        summary_text = "\n".join(report)
        print("\n" + summary_text)
        
        # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        with open("comparison_summary_report.txt", 'w', encoding='utf-8') as f:
            f.write(summary_text)
        print(f"\nğŸ’¾ Summary report saved to: comparison_summary_report.txt")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="XML Batch Comparison Tool - Compare multiple game manuals"
    )
    parser.add_argument(
        "--ai-dir", 
        default="cursor_labeled_integrated",
        help="AIç”ŸæˆXMLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: cursor_labeled_integratedï¼‰"
    )
    parser.add_argument(
        "--human-dir",
        default="output_images_xml_golden",
        help="äººæ‰‹ä½œæˆXMLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: output_images_xml_goldenï¼‰"
    )
    parser.add_argument(
        "--game-ids",
        nargs='+',
        default=None,
        help="æ¯”è¼ƒã™ã‚‹ã‚²ãƒ¼ãƒ IDã®ãƒªã‚¹ãƒˆï¼ˆçœç•¥æ™‚ã¯è‡ªå‹•æ¤œå‡ºï¼‰"
    )
    parser.add_argument(
        "--auto",
        action="store_true",
        help="human_dirã‹ã‚‰_human.xmlãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•æ¤œå‡ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‹•ä½œï¼‰"
    )
    
    args = parser.parse_args()
    
    # ãƒãƒƒãƒæ¯”è¼ƒã‚’å®Ÿè¡Œï¼ˆgame_idsãŒNoneã®å ´åˆã¯è‡ªå‹•æ¤œå‡ºï¼‰
    batch_comparator = XMLBatchComparison(args.ai_dir, args.human_dir, args.game_ids)
    batch_comparator.run_batch_comparison()


if __name__ == "__main__":
    main()

